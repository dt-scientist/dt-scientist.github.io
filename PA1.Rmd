---
title: "Machine learning algorithm"
author: "dt-scientist"
date: "Saturday, August 23, 2014"
output: html_document
---
**Background**

Six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes. 

The goal of this project is to build a model that predicts the Class variable.

The data for this project come from this source: [http://groupware.les.inf.puc-rio.br/har]

**Load and parse the data with the following code:** 

```{r, echo=TRUE, message=FALSE,results='hide'}
# Load the necessary libraries
library(caret)
library(Hmisc)
```

```{r message=FALSE, warning=FALSE}
# train_url<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
# test_url <-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
# download.file(train_url, destfile = "pml-training.csv")
# download.file(test_url,  destfile = "pml-testing.csv")

traindata <- read.csv("pml-training.csv",na.strings=c("NA","","<NA>","NULL"),stringsAsFactors = FALSE,header=TRUE)
testdata <- read.csv("pml-testing.csv",  na.strings=c("NA","","<NA>","NULL"),stringsAsFactors = FALSE,header=TRUE)

traindata$classe <- as.factor(traindata$classe)  

#remove columns that are entirely NA values
traindata<-traindata[,colSums(is.na(traindata)) != nrow(traindata)]

# delete columns that have no impact on the prediction algorithm or contain no data. column X stores the row number.
drops <- c("X","user_name","raw_timestamp_part_1","raw_timestamp_part_2","cvtd_timestamp","skewness_yaw_belt","kurtosis_yaw_forearm","skewness_yaw_forearm
")
traindata<-traindata[,!(names(traindata) %in% drops)]

# nearZeroVar can be use to find those columns from the data for which variance is near to zero(or zero). So we can reduce the dimension of the data by removing those columns for which varaince is zero, because zero variance columns have unique values. So those column doesn't impact the output at all.
removeColumns <-nearZeroVar(subset(traindata, select=-c(classe))) 
traindata <- traindata[, -removeColumns]

#convert to numeric
traindata[ , -grep("classe", names(traindata)) ] <- sapply(subset(traindata, select=-c(classe)), as.numeric)

# backup dataset
traindata_before_imputing<-traindata

#use impute to fill in missing values. Impute with mean value
traindata[ , -grep("classe", names(traindata)) ]<- apply(subset(traindata, select=-c(classe)), 2, function(x) {impute(x,mean)})
```

The dataset has **`r dim(traindata)[1]`** observations and **`r dim(traindata)[2]`** features.

The random forest algorithm can be computationally slow, especially for the analysis of high dimensional data such as our dataset. In order to overcome this problem, we will remove columns were **SOME values are NA** to increase the computational efficiency and speed. 

``` {r}
traindata<-traindata_before_imputing
NAs <- apply(traindata, 2, function(x) {
     sum(is.na(x))
 })
traindata <- traindata[, which(NAs == 0)]
```

The new dataset has **`r dim(traindata)[1]`** observations and 
**`r dim(traindata)[2]`** features.

**Data splitting Based on the Outcome**

The function createDataPartition is used to create splits of the data. Since **classe** is a factor, the random sampling occurs within each class.
``` {r}
#sub-split trainingdata into a training/test set
trainIndex  <- createDataPartition(y=traindata$classe,times=1,
                              p=0.65, list=FALSE)
training <- traindata[trainIndex ,]
testing <- traindata[-trainIndex ,]
```

Check distibution in original data and partitioned data
```{r}
prop.table(table(training$classe)) * 100
```

```{r}
prop.table(table(testing$classe)) * 100
```

```{r}
prop.table(table(traindata$classe)) * 100
```

**Build model on the training set**

```{r, message=FALSE}

# 1 hour  to run
# use multi-core support
library(doParallel)
registerDoParallel(cores=2)

set.seed(1235)
ctrl <- trainControl(allowParallel = TRUE)
classe.model.rf <- train(training$classe~.,data=training, trControl = ctrl, method="rf")
classe.model.rf
```

plot the random forest model:
```{r}
plot(classe.model.rf, log = "y", lwd = 2, main = "Random forest accuracy", xlab = "Predictors", 
    ylab = "Accuracy")
```
The above graph shows the accuracy of our model with different set of predictors

**Evaluate model on the test set (subset of our original training set)**

The confusion matrix on the testing data set shows the out of sample accuracy to be 99.8%.
The out of sample error is 1-99.8%=0.2%
```{r, message=FALSE, echo=TRUE}
rfPredict <- predict(classe.model.rf,newdata = testing )
confusionMatrix(rfPredict, testing$classe )
```

**Apply the machine learning algorithm to the 20 test cases available in the test data above**

```{r}
predict(classe.model.rf, testdata)
```


